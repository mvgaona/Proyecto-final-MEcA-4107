dplyr,
readr,
gamlr,
tidymodels,
ggplot2,
scales,
rvest,
stringr,
boot,
modeest,
stargazer,
sf,
leaflet,
tmaptools,
class,
rgeos,
nngeo,
osmdata,
randomForest,
xgboost,
nnls,
forecast,
zoo,
caret,
BiocManager,
data.table,
plyr,
ranger, SuperLearner)
#packageurl <-"https://cran.r-project.org/src/contrib/Archive/caret/caret_6.0-80.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
#Cargar datos
datacomp <- economics
data<- economics%>% dplyr::select(date, unemploy)
#Generación de valores de índice para el pronóstico. Que sea una predicción de 12 meses.
extended_data <- data %>%
rbind(tibble::tibble(date = seq(from = lubridate::as_date("2015-05-01"),
by = "month", length.out = 12),
unemploy = rep(NA, 12)))
tail(extended_data)
#Dataframe ya tiene las fechas para el pronóstico
#Toca ocuparse de la columna de fecha. xgboost no aborda bien las columnas de fecha, por lo que debemos dividirlo en varias columnas, describiendo la granularidad del tiempo. En este caso meses y años:
extended_data_mod <- extended_data %>%
dplyr::mutate(.,
months = lubridate::month(date),
years = lubridate::year(date))
#Se dividen los datos en conjunto de entrenamiento y conjunto de predicción:
train <- extended_data_mod[1:nrow(data), ] # initial data
View(train)
View(train)
pred <- extended_data_mod[(nrow(data) + 1):nrow(extended_data), ] # extended time index
View(pred)
View(pred)
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
x_train <- model.matrix(unemploy ~months+years, data =train)[, -1]
y_train <- train$unemploy
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_train, label = y_train) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
watchlist <-list(train=xgb_train, test=xgb_test)
model4<- xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)
predicciones_mod4 <-predict(model4, xgb_test)
View(train)
predicciones_mod4 <-predict(model4, xgb_train)
predicciones_mod4<-data.frame(predicciones_mod4)
View(predicciones_mod4)
View(predicciones_mod4)
View(predicciones_mod4)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
rm(predicciones_mod4)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
predicciones_mod4 <-predict(model4, xgb_train)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
Diferencia_mod4 <- (predicciones_mod4 - train$unemploy)
Diferencia_mod4<-data.frame(Diferencia_mod4)
predicciones_mod4 <-predict(model4, y_train)
x_test <- model.matrix(months+years, data =pred)[, -1]
x_test <- model.matrix(unemploy ~months+years, data =pred)[, -1]
y_test <- pred$unemploy
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_test, label = y_test) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
xgb_test <- xgb.DMatrix(data = x_test, label = y_test) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_test, label = y_test) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
View(x_test)
View(x_test)
x_test <- model.matrix(~months+years, data =pred)[, -1]
y_test <- pred$unemploy
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_test, label = y_test) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
xgb_test <- xgb.DMatrix(data = x_test, label = y_test) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_train, label = y_train) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
watchlist <-list(train=xgb_train, test=xgb_test)
model4<- xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)
predicciones_mod4 <-predict(model4, y_train)
predicciones_mod4 <-predict(model4, xgb_train)
predicciones_mod4<-data.frame(predicciones_mod4)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
Diferencia_mod4 <- (predicciones_mod4 - train$unemploy)
Diferencia_mod4<-data.frame(Diferencia_mod4)
x_train <- model.matrix(unemploy ~months+years, data =train)[, -1]
y_train <- train$unemploy
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_train, label = y_train) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
watchlist <-list(train=xgb_train, test=xgb_test)
model4<- xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)
predicciones_mod4 <-predict(model4, xgb_train)
predicciones_mod4<-data.frame(predicciones_mod4)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
x_train <- model.matrix(unemploy ~months+years, data =train)[, -1]
y_train <- train$unemploy
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_train, label = y_train) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
watchlist <-list(train=xgb_train, test=xgb_test)
model4<- xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)
predicciones_mod4 <-predict(model4, y_train)
predicciones_mod4 <-predict(model4, xgb_test)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
Diferencia_mod4 <- (predicciones_mod4 - train$unemploy)
model4<- xgb.train(data = xgb_train, max.depth = 100, watchlist=watchlist, nrounds = 1000)
predicciones_mod4 <-predict(model4, xgb_test)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
Diferencia_mod4 <- (predicciones_mod4 - train$unemploy)
Diferencia_mod4<-data.frame(Diferencia_mod4)
rm(list = ls()) #Limpia las variables que existan al momento de correr el código
#packageurl <-"https://cran.r-project.org/src/contrib/Archive/caret/caret_6.0-80.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
#Cargar datos
datacomp <- economics
data<- economics%>% dplyr::select(date, unemploy)
#Generación de valores de índice para el pronóstico. Que sea una predicción de 12 meses.
extended_data <- data %>%
rbind(tibble::tibble(date = seq(from = lubridate::as_date("2015-05-01"),
by = "month", length.out = 12),
unemploy = rep(NA, 12)))
tail(extended_data)
#Dataframe ya tiene las fechas para el pronóstico
#Toca ocuparse de la columna de fecha. xgboost no aborda bien las columnas de fecha, por lo que debemos dividirlo en varias columnas, describiendo la granularidad del tiempo. En este caso meses y años:
extended_data_mod <- extended_data %>%
dplyr::mutate(.,
months = lubridate::month(date),
years = lubridate::year(date))
#Se dividen los datos en conjunto de entrenamiento y conjunto de predicción:
train <- extended_data_mod[1:nrow(data), ] # initial data
pred <- extended_data_mod[(nrow(data) + 1):nrow(extended_data), ] # extended time index
#y_train<- xgboost::xgb.DMatrix(as.matrix(train %>%
#                                           dplyr::select(unemploy)))
#PREDICCIÓN XG BOOST
#Con los datos preparados como en un apartado anterior se puede realizar el modelo de la misma forma que si no tratáramos con los datos de series temporales.
#Se necesita proporcionar el espacio de parámetros para ajustar el modelo., expecificando el método de validación cruzada con el número de pliegues y también se habilitan cálculos paralelos.
xgb_trcontrol <- caret::trainControl(
method = "cv",
number = 5,
allowParallel = TRUE,
verboseIter = FALSE,
returnData = FALSE
)
xgb_grid <- base::expand.grid(
list(
nrounds = c(100, 200),
max_depth = c(10, 15, 20), # maximum depth of a tree
colsample_bytree = seq(0.5), # subsample ratio of columns when construction each tree
eta = 0.1, # learning rate
gamma = 0, # minimum loss reduction
min_child_weight = 1,  # minimum sum of instance weight (hessian) needed ina child
subsample = 1 # subsample ratio of the training instances
))
#Ahora se puede construir el modelo, usando árboles
xgb_model <- caret::train(
unemploy ~months+years
data=train,
trControl = xgb_trcontrol,
tuneGrid = xgb_grid,
method = "xgbTree",
nthread = 1)
#Ahora se puede construir el modelo, usando árboles
xgb_model <- caret::train(
unemploy ~months+years,
data=train,
trControl = xgb_trcontrol,
tuneGrid = xgb_grid,
method = "xgbTree",
nthread = 1)
#Se observanlos mejores valores que se eligieron como hiperparámetros:
xgb_model$bestTune
#Se realiza la predicción
xgb_pred <- xgb_model %>% stats::predict(x_pred)
x_pred <- xgboost::xgb.DMatrix(as.matrix(pred %>%
dplyr::select(months, years)))
#Se observanlos mejores valores que se eligieron como hiperparámetros:
xgb_model$bestTune
#Se realiza la predicción
xgb_pred <- xgb_model %>% stats::predict(x_pred)
View(xgb_model)
View(xgb_model)
xgb_pred <- xgb_model %>% stats::predict(x_pred)
## prediction en el train set
fitted <- xgb_model %>%
stats::predict(x_train) %>%
stats::ts(start = zoo::as.yearmon(min(train$date)),
end = zoo::as.yearmon(max(train$date)),
frequency = 12)
x_train <- xgboost::xgb.DMatrix(as.matrix(train %>%
dplyr::select(months, years)))
x_pred <- xgboost::xgb.DMatrix(as.matrix(pred %>%
dplyr::select(months, years)))
## prediction en el train set
fitted <- xgb_model %>%
stats::predict(x_train) %>%
stats::ts(start = zoo::as.yearmon(min(train$date)),
end = zoo::as.yearmon(max(train$date)),
frequency = 12)
xgb_pred <- xgb_model %>% stats::predict(pred)
xgb_pred <- xgb_model %>% stats::predict(train)
xgb_pred <- xgb_model %>% stats::predict(pred)
## prediction en el train set
fitted <- xgb_model %>%
stats::predict(train) %>%
stats::ts(start = zoo::as.yearmon(min(train$date)),
end = zoo::as.yearmon(max(train$date)),
frequency = 12)
# prediction in a form of ts object
xgb_forecast <- xgb_pred %>%
stats::ts(start = zoo::as.yearmon(min(pred$date)),
end = zoo::as.yearmon(max(pred$date)),
frequency = 12)
# prediction in a form of ts object
xgb_forecast <- xgb_pred %>%
stats::ts(start = zoo::as.yearmon(min(pred$date)),
end = zoo::as.yearmon(max(pred$date)),
frequency = 12)
# forecast object
forecast_list <- list(
model = xgb_model$modelInfo,
method = xgb_model$method,
mean = xgb_forecast,
x = ts,
fitted = fitted,
residuals = as.numeric(ts) - as.numeric(fitted)
)
# prediction in a form of ts object
xgb_forecast <- xgb_pred %>%
stats::ts(start = zoo::as.yearmon(min(pred$date)),
end = zoo::as.yearmon(max(pred$date)),
frequency = 12)
# forecast object
forecast_list <- list(
model = xgb_model$modelInfo,
method = xgb_model$method,
mean = xgb_forecast,
x = ts,
fitted = fitted,
residuals = as.numeric(ts) - as.numeric(fitted)
)
# forecast object
forecast_list <- list(
model = xgb_model$modelInfo,
method = xgb_model$method,
mean = xgb_forecast,
x = ts,
fitted = fitted
# residuals = as.numeric(ts) - as.numeric(fitted)
)
class(forecast_list) <- "forecast"
forecast::autoplot(forecast_list)
#Predicción con regresores
p_load(forecast)
x_train <- xgboost::xgb.DMatrix(cbind(
as.matrix(extended_data_mod %>% dplyr::select(months, years)),
reg_train))
forecast::autoplot(forecast_list)
class(forecast_list) <- "forecast"
forecast::autoplot(forecast_list)
View(forecast_list)
View(forecast_list)
class(xgb_forecast)
forecast::autoplot(forecast_list)
forecast_list<-as.Matrix(forecast_list)
forecast_list<-as.matrix(forecast_list)
forecast::autoplot(forecast_list)
# forecast object
forecast_list <- list(
model = xgb_model$modelInfo,
method = xgb_model$method,
mean = xgb_forecast,
x = ts,
fitted = fitted
# residuals = as.numeric(ts) - as.numeric(fitted)
)
forecast_list<-as.dataframe(forecast_list)
class(forecast_list) <- "forecast"
forecast_list<-as.dataframe(forecast_list)
forecast_list<-data.frame(forecast_list)
forecast::autoplot(forecast_list)
View(forecast_list)
View(forecast_list)
View(pred)
View(pred)
vIEW(xgb_pred)
View(xgb_pred)
xgb_pred<-data.frame(xgb_pred)
View(xgb_pred)
View(xgb_pred)
Resultados<-cbind(pred$date,xgb_pred)
View(Resultados)
View(Resultados)
rm(list = ls())
install.packages("pacman") #Instalar librería si no cuenta con esta
install.packages("pacman")
library(pacman) #Llamar librería#Se cargan las librerías a usar en el presente Problem Set
p_load(Matrix,
recipes,
rio,
tidyverse,
glmnet,
dplyr,
readr,
gamlr,
tidymodels,
ggplot2,
scales,
rvest,
stringr,
boot,
modeest,
stargazer,
sf,
leaflet,
tmaptools,
class,
rgeos,
nngeo,
osmdata,
randomForest,
xgboost,
nnls,
forecast,
zoo,
caret,
BiocManager,
data.table,
plyr,
ranger, SuperLearner)
rm(list = ls())
#packageurl <-"https://cran.r-project.org/src/contrib/Archive/caret/caret_6.0-80.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
#Cargar datos
datacomp <- economics
data<- economics%>% dplyr::select(date, unemploy)
#Generación de valores de índice para el pronóstico. Que sea una predicción de 12 meses.
extended_data <- data %>%
rbind(tibble::tibble(date = seq(from = lubridate::as_date("2015-05-01"),
by = "month", length.out = 12),
unemploy = rep(NA, 12)))
tail(extended_data)
#Dataframe ya tiene las fechas para el pronóstico
#Toca ocuparse de la columna de fecha. xgboost no aborda bien las columnas de fecha, por lo que debemos dividirlo en varias columnas, describiendo la granularidad del tiempo. En este caso meses y años:
extended_data_mod <- extended_data %>%
dplyr::mutate(.,
months = lubridate::month(date),
years = lubridate::year(date))
#Se dividen los datos en conjunto de entrenamiento y conjunto de predicción:
train <- extended_data_mod[1:nrow(data), ] # initial data
pred <- extended_data_mod[(nrow(data) + 1):nrow(extended_data), ] # extended time index
pred$unemploy[is.na(pred$unemploy] = 0
pred$unemploy[is.na(pred$unemploy)] = 0
View(pred)
View(pred)
x_train <- model.matrix(unemploy ~months+years, data =train)[, -1]
y_train <- train$unemploy
x_test <- model.matrix(~months+years, data =pred)[, -1]
y_test <- pred$unemploy
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_test, label = y_test) #Como se está haciendo sobre la misma base train, se pone el xgb_test como la misma base train
watchlist <-list(train=xgb_train, test=xgb_test)
model4<- xgb.train(data = xgb_train, max.depth = 100, watchlist=watchlist, nrounds = 1000)
predicciones_mod4 <-predict(model4, xgb_test)
MSE_mod4 <- sqrt(mean((predicciones_mod4-train$unemploy)^2))
Diferencia_mod4 <- (predicciones_mod4 - train$unemploy)
Diferencia_mod4<-data.frame(Diferencia_mod4)
rm(list = ls())
install.packages("pacman") #Instalar librería si no cuenta con esta
install.packages("pacman")
p_load(Matrix,
recipes,
rio,
tidyverse,
glmnet,
dplyr,
readr,
gamlr,
tidymodels,
ggplot2,
scales,
rvest,
stringr,
boot,
modeest,
stargazer,
sf,
leaflet,
tmaptools,
class,
rgeos,
nngeo,
osmdata,
randomForest,
xgboost,
nnls,
forecast,
zoo,
BiocManager,
data.table,
ranger, SuperLearner)
library(pacman) #Llamar librería#Se cargan las librerías a usar en el presente Problem Set
p_load(Matrix,
recipes,
rio,
tidyverse,
glmnet,
dplyr,
readr,
gamlr,
tidymodels,
ggplot2,
scales,
rvest,
stringr,
boot,
modeest,
stargazer,
sf,
leaflet,
tmaptools,
class,
rgeos,
nngeo,
osmdata,
randomForest,
xgboost,
nnls,
forecast,
zoo,
BiocManager,
data.table,
ranger, SuperLearner)
#Cargar datos
datacomp <- economics
data<- economics%>% dplyr::select(date, unemploy)
#Generación de valores de índice para el pronóstico. Que sea una predicción de 12 meses.
extended_data <- data %>%
rbind(tibble::tibble(date = seq(from = lubridate::as_date("2015-05-01"),
by = "month", length.out = 12),
unemploy = rep(NA, 12)))
tail(extended_data)
#Dataframe ya tiene las fechas para el pronóstico
#Toca ocuparse de la columna de fecha. xgboost no aborda bien las columnas de fecha, por lo que debemos dividirlo en varias columnas, describiendo la granularidad del tiempo. En este caso meses y años:
extended_data_mod <- extended_data %>%
dplyr::mutate(.,
months = lubridate::month(date),
years = lubridate::year(date))
#Se dividen los datos en conjunto de entrenamiento y conjunto de predicción:
train <- extended_data_mod[1:nrow(data), ] # initial data
pred <- extended_data_mod[(nrow(data) + 1):nrow(extended_data), ] # extended time index
x_train <- xgboost::xgb.DMatrix(as.matrix(train %>%
dplyr::select(months, years)))
x_pred <- xgboost::xgb.DMatrix(as.matrix(pred %>%
dplyr::select(months, years)))
pred <- extended_data_mod[(nrow(data) + 1):nrow(extended_data), ] # extended time index
pred$unemploy[is.na(pred$unemploy)] = 0
x_train <- xgboost::xgb.DMatrix(as.matrix(train %>%
dplyr::select(months, years)))
x_pred <- xgboost::xgb.DMatrix(as.matrix(pred %>%
dplyr::select(months, years)))
y_train <- train$unemploy
#PREDICCIÓN XG BOOST
#Con los datos preparados como en un apartado anterior se puede realizar el modelo de la misma forma que si no tratáramos con los datos de series temporales.
#Se necesita proporcionar el espacio de parámetros para ajustar el modelo., expecificando el método de validación cruzada con el número de pliegues y también se habilitan cálculos paralelos.
xgb_trcontrol <- caret::trainControl(
method = "cv",
number = 5,
allowParallel = TRUE,
verboseIter = FALSE,
returnData = FALSE
)
xgb_grid <- base::expand.grid(
list(
nrounds = c(100, 200),
max_depth = c(10, 15, 20), # maximum depth of a tree
colsample_bytree = seq(0.5), # subsample ratio of columns when construction each tree
eta = 0.1, # learning rate
gamma = 0, # minimum loss reduction
min_child_weight = 1,  # minimum sum of instance weight (hessian) needed ina child
subsample = 1 # subsample ratio of the training instances
))
#Ahora se puede construir el modelo, usando árboles
xgb_model <- caret::train(
unemploy ~months+years,
data=train,
trControl = xgb_trcontrol,
tuneGrid = xgb_grid,
method = "xgbTree",
nthread = 1)
#Se observanlos mejores valores que se eligieron como hiperparámetros:
xgb_model$bestTune
xgb_pred <- xgb_model %>% stats::predict(pred)
xgb_pred<-data.frame(xgb_pred)
Resultados<-cbind(pred$date,xgb_pred)
data1 <- read.csv("../Datos/Listado_agentes.csv", header=TRUE, stringsAsFactors=FALSE)
setwd("E:/MAESTRIA UNIANDES/BIG DATA/Proyecto-final-MEcA-4107/Script")
data1 <- read.csv("../Datos/Listado_agentes.csv", header=TRUE, stringsAsFactors=FALSE)
View(data1)
View(data1)
